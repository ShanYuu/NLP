{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FdDgY9lnVchn"
   },
   "source": [
    "# Chapter 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "u_v2e74QA-5V",
    "outputId": "6987db08-ae1d-475d-e96b-9ec18db95597"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# page72\n",
    "from nltk.tokenize import TreebankWordTokenizer #導入TreebankWordTokenizer\n",
    "sentence = \"\"\"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\" #定義sentence\n",
    "tokenizer = TreebankWordTokenizer() \n",
    "tokens = tokenizer.tokenize(sentence.lower()) #利用TreebankWordTokenizer將sentence中的每個字分開成token，利用lower將每個單字的大寫轉換成小寫\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "x7m4kx0URgAg",
    "outputId": "f901cfb0-cf8d-4727-ee32-55ece1ebcb87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag_of_words = Counter(tokens) #利用Counter可以計算每個token在sentence中出現的次數，並將他們整合成bag_of_words\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "liKUDToQRidP",
    "outputId": "32c47ebe-4c33-45a4-f139-bb338082ea54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4) #透過most.common(4)可以找出在bag_of_word中的token出現頻率最高的前四個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3HVICYImRkwQ",
    "outputId": "67006ccf-cc29-4bc7-fee7-8150aebb0c2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_harry_appears = bag_of_words['harry'] #找出'harry'在bag_of_word中的次數定義成times_harry_appears\n",
    "num_unique_words = len(bag_of_words) #計算bag_of_word的長度並定義成num_unique_words\n",
    "tf = times_harry_appears / num_unique_words #tf計算harry出現的次數在bag_of_word中全部所佔的權重是多少\n",
    "round(tf, 4) #四捨五入到小數第四位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "D35BIByQWBFS",
    "outputId": "35b96ffd-b81f-42fa-e4e0-5c2f0d4f57e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpia\n",
      "  Using cached nlpia-0.2.14-py2.py3-none-any.whl (32.0 MB)\n",
      "Requirement already satisfied: seaborn in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (0.10.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.1.0-cp37-cp37m-macosx_10_11_x86_64.whl (120.8 MB)\n",
      "Requirement already satisfied: h5py in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (2.10.0)\n",
      "Collecting spacy\n",
      "  Downloading spacy-2.2.4-cp37-cp37m-macosx_10_9_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: lxml in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (4.4.2)\n",
      "Requirement already satisfied: matplotlib in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (3.1.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (0.22.1)\n",
      "Collecting html2text\n",
      "  Using cached html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
      "Collecting pandas-datareader\n",
      "  Using cached pandas_datareader-0.8.1-py2.py3-none-any.whl (107 kB)\n",
      "Requirement already satisfied: jupyter in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (1.0.0)\n",
      "Collecting pugnlp\n",
      "  Using cached pugnlp-0.2.5-py2.py3-none-any.whl (706 kB)\n",
      "Requirement already satisfied: pandas in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (1.0.0)\n",
      "Collecting gensim\n",
      "  Using cached gensim-3.8.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7 MB)\n",
      "Collecting plotly\n",
      "  Downloading plotly-4.6.0-py2.py3-none-any.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (4.42.0)\n",
      "Collecting keras\n",
      "  Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "Requirement already satisfied: scipy in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (1.3.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.4.4.tar.gz (695 kB)\n",
      "\u001b[K     |████████████████████████████████| 695 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-Levenshtein\n",
      "  Using cached python-Levenshtein-0.12.0.tar.gz (48 kB)\n",
      "Requirement already satisfied: nltk in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (3.4.5)\n",
      "Processing /Users/sandy/Library/Caches/pip/wheels/3e/55/4f/59e0fa0914f3db52e87c0642c5fb986871dfbbf253026e639f/pypandoc-1.4-cp37-none-any.whl\n",
      "Requirement already satisfied: future in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nlpia) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from seaborn->nlpia) (1.18.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from tensorflow->nlpia) (0.34.1)\n",
      "Processing /Users/sandy/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6/termcolor-1.1.0-cp37-none-any.whl\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.28.1-cp37-cp37m-macosx_10_9_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from tensorflow->nlpia) (1.14.0)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.11.3-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Using cached tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from tensorflow->nlpia) (1.11.2)\n",
      "Processing /Users/sandy/Library/Caches/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d/absl_py-0.9.0-cp37-none-any.whl\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Processing /Users/sandy/Library/Caches/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp37-none-any.whl\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 282 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from spacy->nlpia) (2.22.0)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Using cached blis-0.4.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.0 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.3-cp37-cp37m-macosx_10_6_intel.whl (54 kB)\n",
      "Collecting thinc==7.4.0\n",
      "  Downloading thinc-7.4.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: setuptools in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from spacy->nlpia) (45.1.0.post20200127)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Using cached wasabi-0.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.2-cp37-cp37m-macosx_10_9_x86_64.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 20.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (211 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->nlpia) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->nlpia) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->nlpia) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->nlpia) (0.10.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->nlpia) (0.14.1)\n",
      "Requirement already satisfied: jupyter-console in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter->nlpia) (6.1.0)\n",
      "Requirement already satisfied: notebook in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter->nlpia) (6.0.3)\n",
      "Requirement already satisfied: ipykernel in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter->nlpia) (5.1.4)\n",
      "Requirement already satisfied: nbconvert in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter->nlpia) (5.6.1)\n",
      "Requirement already satisfied: qtconsole in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter->nlpia) (4.6.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter->nlpia) (7.5.1)\n",
      "Processing /Users/sandy/Library/Caches/pip/wheels/11/94/81/312969455540cb0e6a773e5d68a73c14128bfdfd4a7969bb4f/python_slugify-4.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pip in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from pugnlp->nlpia) (20.0.2)\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting coverage\n",
      "  Downloading coverage-5.0.4-cp37-cp37m-macosx_10_13_x86_64.whl (203 kB)\n",
      "\u001b[K     |████████████████████████████████| 203 kB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from pandas->nlpia) (2019.3)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-1.11.1.tar.gz (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hProcessing /Users/sandy/Library/Caches/pip/wheels/d7/a9/33/acc7b709e2a35caa7d4cae442f6fe6fbf2c43f80823d46460c/retrying-1.3.3-cp37-none-any.whl\n",
      "Requirement already satisfied: pyyaml in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from keras->nlpia) (5.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->nlpia) (0.16.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 6.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.13.1-py2.py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 6.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->nlpia) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->nlpia) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->nlpia) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->nlpia) (1.25.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy->nlpia) (1.4.0)\n",
      "Requirement already satisfied: ipython in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter-console->jupyter->nlpia) (7.11.1)\n",
      "Requirement already satisfied: jupyter-client in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter-console->jupyter->nlpia) (5.3.4)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter-console->jupyter->nlpia) (3.0.3)\n",
      "Requirement already satisfied: pygments in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jupyter-console->jupyter->nlpia) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (2.10.3)\n",
      "Requirement already satisfied: prometheus-client in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (0.7.1)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (4.3.3)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (4.6.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (0.8.3)\n",
      "Requirement already satisfied: Send2Trash in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (1.5.0)\n",
      "Requirement already satisfied: ipython-genutils in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (0.2.0)\n",
      "Requirement already satisfied: tornado>=5.0 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (6.0.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (18.1.0)\n",
      "Requirement already satisfied: nbformat in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from notebook->jupyter->nlpia) (5.0.4)\n",
      "Requirement already satisfied: appnope; platform_system == \"Darwin\" in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from ipykernel->jupyter->nlpia) (0.1.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->nlpia) (0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->nlpia) (0.8.4)\n",
      "Requirement already satisfied: testpath in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->nlpia) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->nlpia) (1.4.2)\n",
      "Requirement already satisfied: bleach in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->nlpia) (3.1.0)\n",
      "Requirement already satisfied: defusedxml in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->jupyter->nlpia) (0.6.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from ipywidgets->jupyter->nlpia) (3.5.1)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: boto in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim->nlpia) (2.49.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.12.39-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Using cached rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->nlpia) (0.6.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-console->jupyter->nlpia) (0.16.0)\n",
      "Requirement already satisfied: decorator in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-console->jupyter->nlpia) (4.4.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-console->jupyter->nlpia) (4.8.0)\n",
      "Requirement already satisfied: backcall in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-console->jupyter->nlpia) (0.1.0)\n",
      "Requirement already satisfied: pickleshare in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-console->jupyter->nlpia) (0.7.5)\n",
      "Requirement already satisfied: wcwidth in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->nlpia) (0.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jinja2->notebook->jupyter->nlpia) (1.1.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from nbformat->notebook->jupyter->nlpia) (3.2.0)\n",
      "Requirement already satisfied: webencodings in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert->jupyter->nlpia) (0.5.1)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.16.0,>=1.15.39\n",
      "  Downloading botocore-1.15.39-py2.py3-none-any.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: more-itertools in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->nlpia) (8.0.2)\n",
      "Requirement already satisfied: parso>=0.5.2 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython->jupyter-console->jupyter->nlpia) (0.6.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->jupyter-console->jupyter->nlpia) (0.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter->nlpia) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter->nlpia) (0.15.7)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Building wheels for collected packages: regex, python-Levenshtein, smart-open\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2020.4.4-cp37-cp37m-macosx_10_9_x86_64.whl size=286754 sha256=cc2a7aedc7fa88e437d7c67ba8a9400540ccd2028c4de5adc9d8f7ea647088ff\n",
      "  Stored in directory: /Users/sandy/Library/Caches/pip/wheels/52/98/5c/8d5d7a5321d2de463dfc6f7b32aab8b16e564cfce553b28daf\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp37-cp37m-macosx_10_9_x86_64.whl size=79411 sha256=1e714711660176d6ff94afe51133bda4ff3fc0d7adbbb4ff3ccc0f8793fc9479\n",
      "  Stored in directory: /Users/sandy/Library/Caches/pip/wheels/f0/9b/13/49c281164c37be18343230d3cd0fca29efb23a493351db0009\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-1.11.1-py3-none-any.whl size=95255 sha256=ab824e0e75db1d47c14ca7392590ce64c2681789a695703965e60fce06011c7d\n",
      "  Stored in directory: /Users/sandy/Library/Caches/pip/wheels/1a/8c/a2/7b24df77c58dab0aec275c1bd3d4392d54df941020199f6759\n",
      "Successfully built regex python-Levenshtein smart-open\n",
      "\u001b[31mERROR: tensorflow 2.1.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.3.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: termcolor, keras-applications, google-pasta, grpcio, opt-einsum, protobuf, astor, tensorflow-estimator, absl-py, keras-preprocessing, gast, markdown, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow, blis, cymem, murmurhash, preshed, catalogue, plac, wasabi, srsly, thinc, spacy, html2text, pandas-datareader, text-unidecode, python-slugify, python-Levenshtein, fuzzywuzzy, coverage, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim, pypandoc, retrying, plotly, pugnlp, keras, regex, nlpia\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 blis-0.4.1 boto3-1.12.39 botocore-1.15.39 cachetools-4.1.0 catalogue-1.0.0 coverage-5.0.4 cymem-2.0.3 docutils-0.15.2 fuzzywuzzy-0.18.0 gast-0.2.2 gensim-3.8.1 google-auth-1.13.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.28.1 html2text-2020.1.16 jmespath-0.9.5 keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 murmurhash-1.0.2 nlpia-0.2.14 oauthlib-3.1.0 opt-einsum-3.2.0 pandas-datareader-0.8.1 plac-1.1.3 plotly-4.6.0 preshed-3.0.2 protobuf-3.11.3 pugnlp-0.2.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 pypandoc-1.4 python-Levenshtein-0.12.0 python-slugify-4.0.0 regex-2020.4.4 requests-oauthlib-1.3.0 retrying-1.3.3 rsa-4.0 s3transfer-0.3.3 smart-open-1.11.1 spacy-2.2.4 srsly-1.0.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 text-unidecode-1.3 thinc-7.4.0 wasabi-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CQClCH6FRntV",
    "outputId": "4e8f28f5-6b30-4838-b632-16a09d987bc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "/Users/sandy/opt/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:87: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/sandy/opt/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:137: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "/Users/sandy/opt/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:159: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "/Users/sandy/opt/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:167: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  INF = pd.np.inf\n",
      "/Users/sandy/opt/anaconda3/lib/python3.7/site-packages/pugnlp/constants.py:168: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  NAN = pd.np.nan\n",
      "/Users/sandy/opt/anaconda3/lib/python3.7/site-packages/pugnlp/tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/sandy/opt/anaconda3/lib/python3.7/site-packages/pugnlp/util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/sandy/opt/anaconda3/lib/python3.7/site-packages/nlpia/futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "/Users/sandy/opt/anaconda3/lib/python3.7/site-packages/nlpia/loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'a': 20,\n",
       "         'kite': 16,\n",
       "         'is': 7,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'with': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'that': 2,\n",
       "         'react': 1,\n",
       "         'against': 1,\n",
       "         'the': 26,\n",
       "         'air': 2,\n",
       "         'to': 5,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'and': 10,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'of': 10,\n",
       "         'wings': 1,\n",
       "         ',': 15,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'have': 4,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'at': 3,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'so': 3,\n",
       "         'wind': 2,\n",
       "         'can': 3,\n",
       "         'it.': 1,\n",
       "         \"'s\": 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'be': 5,\n",
       "         'designed': 2,\n",
       "         'not': 1,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'when': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'for': 2,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'or': 6,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'in': 7,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'above': 1,\n",
       "         'high': 1,\n",
       "         'below': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'from': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'by': 2,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'more': 1,\n",
       "         'lines': 1,\n",
       "         'which': 2,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'e.g.': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'as': 5,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'same': 1,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'are': 3,\n",
       "         'used': 2,\n",
       "         'under': 1,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'both': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'other': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'such': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'been': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# page75\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "from nlpia.data.loaders import kite_text\n",
    "tokens = tokenizer.tokenize(kite_text.lower()) #從nlpia的資料裡導入kite_text的資料，並透過TreebankWordTokenizer將資料每個字都變成token，並將大寫轉成小寫\n",
    "token_counts = Counter(tokens) #計算每個token出現的次數\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CwTihe8vRtS0",
    "outputId": "706dc0ce-036d-46e3-ddd8-ae4cbea4f8f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True) #nltk.download下載stopwords並回傳安裝成功的話顯示True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0qDELbViSJOV",
    "outputId": "b4775803-bf38-4093-f9a3-45fdfb897965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'kite': 16,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'react': 1,\n",
       "         'air': 2,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'wings': 1,\n",
       "         ',': 15,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'wind': 2,\n",
       "         'it.': 1,\n",
       "         \"'s\": 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'designed': 2,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'high': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'lines': 1,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'e.g.': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'used': 2,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english') # 引用 NLTK 英文版本的 stopwords\n",
    "tokens = [ x for x in tokens if x not in stopwords] #將不是在stopwords裡的詞儲存在一個list裡面\n",
    "kite_counts = Counter(tokens) #接著計算每個token在list裡面出現的次數\n",
    "kite_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "L8Jb3LFGSJqh",
    "outputId": "8173802f-cd9c-418d-bdd4-9f1d959a959d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07207207207207207,\n",
       " 0.06756756756756757,\n",
       " 0.036036036036036036,\n",
       " 0.02252252252252252,\n",
       " 0.018018018018018018,\n",
       " 0.018018018018018018,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens) #計算kite這篇文章的長度\n",
    "for key, value in kite_counts.most_common(): document_vector.append(value / doc_length)\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aP5vlS2QdX_L"
   },
   "outputs": [],
   "source": [
    " docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"] #先定義docs\n",
    " docs.append(\"Harry is hairy and faster than Jill.\") #在docs後面分別加上句子\n",
    " docs.append(\"Jill is not as hairy as Harry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TcvdcjgIlbXG",
    "outputId": "e3e6833a-0ca9-4fe6-9093-0d50308f4ffe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens = []\n",
    "for doc in docs : doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))] \n",
    "#先將docs中的大寫轉換成小寫以及將其單詞轉換成token，接著使用sotrted依照字母順序做排列\n",
    "len(doc_tokens[0]) #計算出docs中第0個list的長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gAADKZshlhXa",
    "outputId": "2cc0be58-c496-43b5-a0a0-15febec91762"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens, []) \n",
    "len(all_doc_tokens) #計算出所有doc_tokens的tokens數量，並列出其長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6XXPGk4DnFj6",
    "outputId": "34e58139-443e-4d13-a3f8-8688705844c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = sorted(set(all_doc_tokens)) #計算出在all_doc_tokens中沒有順序且不重複的token進行排列並依照字母做排序 \n",
    "len(lexicon) #列出其長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "4FGNTOCgnI2S",
    "outputId": "39bfc973-1881-4c18-cbc0-b0120469f1a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "KNQnXs3MnPoM",
    "outputId": "0ac1c6f6-a33d-42ba-cba1-c802a2ea4089"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict #使用OrderedDict會根據放入元素的先後順序進行排序\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon) #將每個token加入向量，並使用OrderedDict做先後順序的排序\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dw6erXkjnoUT"
   },
   "outputs": [],
   "source": [
    "import copy #!!!copy其差別\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "  vec = copy.copy(zero_vector) #複製\n",
    "  tokens = tokenizer.tokenize(doc.lower())\n",
    "  token_counts = Counter(tokens)\n",
    "  for key, value in token_counts.items():\n",
    "    vec[key] = value / len(lexicon)\n",
    "  doc_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDiD35HNn_0X"
   },
   "outputs": [],
   "source": [
    "# page 82\n",
    "# 3.1 Compute cosine similarity in python\n",
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "  \"\"\" Let's convert our dictionaries to lists for easier matching.\"\"\"\n",
    "  vec1 = [val for val in vec1.values()] \n",
    "  vec2 = [val for val in vec2.values()]\n",
    "\n",
    "  dot_prod = 0\n",
    "  for i, v in enumerate(vec1): # !!!enumerate\n",
    "    dot_prod += v * vec2[i]\n",
    "  \n",
    "  mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "  mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "\n",
    "  return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "PxIF_x45vTy6",
    "outputId": "7ecfe603-a07d-4c61-f1f7-2f9f3ed659c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/sandy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# page 85\n",
    "nltk.download('brown') #下載在nltk資料庫裡的brown\n",
    "from nltk.corpus import brown #透過nltk中的資料庫查找brown\n",
    "brown.words()[:10] #列出在browm中前10個字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "aKUWpJllzHq5",
    "outputId": "5f68b442-0597-4beb-e8dd-c11b54e98e30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[:5] #標示其brown前五個單字的詞性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sjBkdHyQzMKF",
    "outputId": "192cb4e3-6f80-4c8c-b827-6e3fd513194d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words()) #計算brown的長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "bs6CddPyzXbB",
    "outputId": "661e9dd5-040e-412c-dbcf-6774d8f6b54d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548),\n",
       " ('for', 9489),\n",
       " ('it', 8760),\n",
       " ('with', 7289),\n",
       " ('as', 7253),\n",
       " ('his', 6996),\n",
       " ('on', 6741),\n",
       " ('be', 6377),\n",
       " ('at', 5372),\n",
       " ('by', 5306),\n",
       " ('i', 5164)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "puncs = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
    "word_list = (x.lower() for x in brown.words() if x not in puncs) #先將其字母變換成小寫，接著找出在brown.words()中沒有在puncs裡的存成word_list\n",
    "token_counts = Counter(word_list) #計算word_list裡的token出現頻率\n",
    "token_counts.most_common(20) # 列出word_list中最常出現的前20個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z1a-4F69zksx",
    "outputId": "9ff93e17-a9ed-48bc-8e60-316a802a67dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlpia.data.loaders import kite_text, kite_history\n",
    "kite_intro = kite_text.lower() #將kite_text轉換成小寫\n",
    "intro_tokens = tokenizer.tokenize(kite_intro) #換成小寫後分開裡面的單詞,儲存成intro_tokens\n",
    "kite_history = kite_history.lower() #將kite_history轉換成小寫\n",
    "history_tokens = tokenizer.tokenize(kite_history) #換成小寫後分開裡面的單詞，儲存成history_tokens\n",
    "intro_total = len(intro_tokens) #計算其intro_tokens的長度\n",
    "intro_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_54U-kUXD7BC",
    "outputId": "710c75a1-13e1-4648-cc9c-eba6cdc96ca8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_total = len(history_tokens)#計算其history_tokens的長度\n",
    "history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Yk62bPqFEPJm",
    "outputId": "36224738-5329-455e-cecc-07ff97aaac7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Term Frequency of \"kite\" in intro is: 0.0441'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "intro_counts = Counter(intro_tokens) #計算intro_tokens每個token的出現次數\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_total #在intro中kite的頻率 算出比重\n",
    "history_counts = Counter(history_tokens) #計算history_tokens每個token的出現次數\n",
    "history_tf['kite'] = history_counts['kite'] / history_total #在histro中kite的頻率 算出比重\n",
    "'Term Frequency of \"kite\" in intro is: {:.4f}'.format(intro_tf['kite']) # 將 intro中kite tf的值以列出小數後四位的方式顯示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LlMjlrOyKU7_",
    "outputId": "80e703f4-240d-48bb-a9ae-4ad9414ccaf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Term Frequency of \"kite\" in history is: 0.0202'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Term Frequency of \"kite\" in history is: {:.4f}'.format(history_tf['kite'])# 將history中kite tf的值以列出小數後四位的方式顯示 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H6fuusOIKqkf",
    "outputId": "740518bb-bb4c-47ba-ac49-8387d7895d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"and\" in intro is: 0.0275\n"
     ]
    }
   ],
   "source": [
    "# page87\n",
    "intro_tf['and'] = intro_counts['and'] / intro_total  #在intro中and的頻率，算出比重\n",
    "history_tf['and'] = history_counts['and'] / history_total #在histroy中and的頻率，算出比重\n",
    "print('Term Frequency of \"and\" in intro is: {:.4f}'.format(intro_tf['and'])) #將 intro中and的tf值以列出小數後四位的方式顯示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "haEeukqjL7b0",
    "outputId": "ffb7aedc-291a-4b80-aaef-f9fac5611c29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"and\" in history is: 0.0303\n"
     ]
    }
   ],
   "source": [
    "print('Term Frequency of \"and\" in history is: {:.4f}'.format(history_tf['and'])) #將 history中and的tf值以列出小數後四位的方式顯示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1pjRdDYMIFY"
   },
   "outputs": [],
   "source": [
    "# page 88\n",
    "num_docs_containing_and = 0\n",
    "for doc in [intro_tokens, history_tokens]: #and在兩個文件中每出現一次就+1\n",
    "  if 'and' in doc:\n",
    "    num_docs_containing_and += 1\n",
    "\n",
    "num_docs_containing_kite = 0\n",
    "for doc in [intro_tokens, history_tokens]: #kite在兩個文件中每出現一次就+1\n",
    "  if 'kite' in doc:\n",
    "    num_docs_containing_kite += 1\n",
    "\n",
    "num_docs_containing_china= 0\n",
    "for doc in [intro_tokens, history_tokens]: #china在兩個文件中每出現一次就+1\n",
    "  if 'china' in doc:\n",
    "    num_docs_containing_china += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "siSJqySZRdU4"
   },
   "outputs": [],
   "source": [
    "intro_tf['china'] = intro_counts['china'] / intro_total #在intro中china的頻率，算出比重\n",
    "history_tf['china'] = history_counts['china'] / history_total #在histroy中china的頻率，算出比重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bpoQQ-rRhWw"
   },
   "outputs": [],
   "source": [
    "num_docs = 2 #文檔數量=2\n",
    "intro_idf = {} \n",
    "history_idf = {}\n",
    "intro_idf['and'] = num_docs / num_docs_containing_and #文件數量/在2個資料中出現and的總數 並存成idf\n",
    "history_idf['and'] = num_docs / num_docs_containing_and #文件數量/在2個資料中出現and的總數 並存成idf\n",
    "intro_idf['kite'] = num_docs / num_docs_containing_kite #文件數量/在2個資料中出現kite的總數 並存成idf\n",
    "history_idf['kite'] = num_docs / num_docs_containing_kite #文件數量/在2個資料中出現kite的總數 並存成idf\n",
    "intro_idf['china'] = num_docs / num_docs_containing_china #文件數量/在2個資料中出現china的總數 並存成idf\n",
    "history_idf['china'] = num_docs / num_docs_containing_china #文件數量/在2個資料中出現china的總數 並存成idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I0KbwtMsTae8",
    "outputId": "9aea853e-47f2-4e62-e75a-e1ab72c108ba"
   },
   "outputs": [],
   "source": [
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and'] # 計算出intro_tfidf['and']，and在intro文件中出現的頻率＊（文件數量/在2個資料中出現and的總數）\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite'] # 計算出intro_tfidf['kite']，kite在intro文件中出現的頻率＊（文件數量/在2個資料中出現kite的總數）\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china'] # 計算出intro_tfidf['china']，china在intro文件中出現的頻率＊（文件數量/在2個資料中出現china的總數）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kK1bS7F7UoMv"
   },
   "outputs": [],
   "source": [
    "# page 89\n",
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and'] * history_idf['and'] # 計算出history_tfidf['and']，and在history文件中出現的頻率＊（文件數量/在2個資料中出現and的總數）\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite'] #  計算出history_tfidf['kite']，kite在history文件中出現的頻率＊（文件數量/在2個資料中出現kite的總數）\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china'] #  計算出history_tfidf['china']，china在history文件中出現的頻率＊（文件數量/在2個資料中出現china的總數）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "R3-HRFTeWdQy",
    "outputId": "a987e158-b61e-4377-87f0-d9a3fa6ddaf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china的次數: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4.961632770716254"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "print('china的次數:', history_counts['china']) # 顯示出在history_counts中china的次數\n",
    "log_tf = log(history_counts['china']) - log(history_total) #將 history_counts中china的次數使用log\n",
    "log_idf = log(log(num_docs) - log(num_docs_containing_china))\n",
    "log_tf_idf = log_tf + log_idf\n",
    "log_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdnQqLzVY1Ad"
   },
   "outputs": [],
   "source": [
    "# page 91\n",
    "document_tfidf_vectors = [] #建立list\n",
    "documents = docs\n",
    "for doc in documents:\n",
    "    vec = copy.copy(zero_vector) # 複製一份zero_vector做引用\n",
    "    tokens = tokenizer.tokenize(doc.lower()) # 將docs的資料轉換為小寫後，用tokenizer將句子切割成單字\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf * idf\n",
    "    document_tfidf_vectors.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "acWYD2Hmaloj",
    "outputId": "95565c20-eb04-4f53-9149-8881c69dbfdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How long does it take to get to the store?\"\n",
    "query_vec = copy.copy(zero_vector) # 複製一份zero_vector做引用\n",
    "\n",
    "query_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "qwPeKR9Qa-3Z",
    "outputId": "c59f123d-f3f8-4689-a545-84da48679a51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6132857433407973"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(query.lower())\n",
    "#將query句子轉換為小寫後，用tokenizer將句子切割成單字，儲存成tokens\n",
    "token_counts = Counter(tokens) #Counter()計算tokens中每個字出現的次數\n",
    "\n",
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    for _doc in documents:\n",
    "        if key in _doc.lower():\n",
    "            docs_containing_key += 1\n",
    "    if docs_containing_key == 0:\n",
    "        continue\n",
    "    tf = value / len(tokens)\n",
    "    idf = len(documents) / docs_containing_key\n",
    "    query_vec[key] = tf * idf\n",
    "cosine_sim(query_vec, document_tfidf_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "wEIVWbzPbUIZ",
    "outputId": "6aebd5bf-8852-4a2a-ea00-8bd5e621186e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJw9qaD2cc-5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ii3VmORQcp4J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (1.3.1)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/sandy/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=a6e098d3eedeb404f895c5b34e6293dc32a1fc18c6b16541152e53f2280788a1\n",
      "  Stored in directory: /Users/sandy/Library/Caches/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gw3jVCGQddUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16 0.   0.48 0.21 0.21 0.   0.25 0.21 0.   0.   0.   0.21 0.   0.64\n",
      "  0.21 0.21]\n",
      " [0.37 0.   0.37 0.   0.   0.37 0.29 0.   0.37 0.37 0.   0.   0.49 0.\n",
      "  0.   0.  ]\n",
      " [0.   0.75 0.   0.   0.   0.29 0.22 0.   0.29 0.29 0.38 0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "print(model.todense().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
